{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1124104-fbdb-40a1-a9b0-4c0b0c5ca7db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Overview\n",
    "Based on the `all_s3_records` column from the `silver_flat_repositories` table, load csvs into tables. Some remarks:\n",
    "\n",
    "* Doing this per-country\n",
    "* Only csvs will be processed\n",
    "* Catalog and database name will be inferred based on the filepath of the s3. For example, `/indonesia/trade/../**.csv` will assume the table should be loaded in a 'trade' db within an 'indonesia' catalog. If they don't exist, they will be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11c25eb1-e9aa-46f0-a2bf-c8f13d96020c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "import re\n",
    "import ntpath\n",
    "\n",
    "data_bucket = \"s3a://****\"\n",
    "catalog_bucket = \"s3a://****\"\n",
    "\n",
    "flat_repositories = \"****.default.silver_flat_repositories\"\n",
    "col_with_s3s = \"all_s3_records\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51a781d2-36a5-460c-aeae-d481b03a5fdb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Main functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33cc2372-0614-4b13-8b6e-d71c7c83d3f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## General supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a34108e2-e4d0-4332-ac77-65f5f2692e34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def find_delimiter(header_text):\n",
    "    \"\"\"Given a header, returns the first ocurrence of a delimiter character from |;,\\t \"\"\"\n",
    "    delimiters = [\";\",\",\",\"\\t\",\"|\"]\n",
    "    del_index = next((i for i,c in enumerate(header_text) if c in delimiters))\n",
    "    return(header_text[del_index])\n",
    "\n",
    "def get_non_alphanumeric_chars(string):\n",
    "    non_alphanumeric_chars = set(re.findall(r'[^a-zA-Z0-9]', string))\n",
    "    allowed_subet = set([\";\",\",\",\"\\t\",\"|\",\" \",\"_\",\"\\r\",\"\\n\"])\n",
    "    return list(non_alphanumeric_chars - allowed_subet)\n",
    "\n",
    "def unity_path_from_filename(file_path):\n",
    "    \"\"\"Based on a file path, return the first folder as catalog and subfolder as db name\"\"\"\n",
    "    catalog, db = ntpath.dirname(file_path).split('/')[3:5]\n",
    "    return catalog, db\n",
    "\n",
    "def create_temp_csv(new_content, file_name, catalog_bucket):\n",
    "    \"\"\"Creates a temporary csv based on the string provided and places it in s3:{catalog_bucket}/tmp/file_name_TSTIMESTAMP.csv\"\"\"\n",
    "    current_time = int(time.time())\n",
    "    filepath = f\"{catalog_bucket}/tmp/{file_name}_TS{current_time}.csv\"\n",
    "    dbutils.fs.put(filepath, new_content)\n",
    "    return(filepath)\n",
    "\n",
    "def generate_table_name(csv_file_name):\n",
    "    \"\"\"Returns a table name with a 'bronze_' prefix, and replacing special characters\"\"\"\n",
    "    table_name = re.sub('[^0-9a-zA-Z]+', '_', csv_file_name).lower().removesuffix(\"_csv\")\n",
    "    return(\"bronze_\"+table_name)\n",
    "\n",
    "def create_catalog(catalog_name, catalog_bucket, comment=\"\"):\n",
    "    location = f\"{catalog_bucket}/{catalog_name}\"\n",
    "    comment = f\"{catalog_name} catalog, hosted in {location}. {comment}\"\n",
    "    spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name} \" \\\n",
    "    f\" MANAGED LOCATION '{location}' \" \\\n",
    "    f\" COMMENT \\\"{comment}\\\" \")\n",
    "    return True\n",
    "\n",
    "def create_db(db_name, catalog_name, catalog_bucket, comment=\"\"):\n",
    "    location = f\"{catalog_bucket}/{catalog_name}/{db_name}\"\n",
    "    comment = f\"{db_name} database, hosted in {location}. {comment}\"\n",
    "    spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name} \" \\\n",
    "    f\" MANAGED LOCATION '{location}' \" \\\n",
    "    f\" COMMENT \\\"{comment}\\\" \")\n",
    "    return True\n",
    "\n",
    "def file_exists(file_path):\n",
    "    \"\"\"Given a path, it checks if the file exists and returns a boolean\"\"\"\n",
    "    try:  \n",
    "        # Your logic here  \n",
    "        output = dbutils.fs.ls(file_path)\n",
    "        return_bool = True  \n",
    "    except Exception as e:  \n",
    "        output = f\"{e}\"  \n",
    "        return_bool = False \n",
    "    return (return_bool, output)  \n",
    "\n",
    "def list_countries_csvs(country, flat_repositories, col_with_s3s, data_bucket, db=\"\"):\n",
    "    \"\"\"Given a country and a reference to the repositories table,\"\"\"\n",
    "    df_s3s = spark.sql(f\"SELECT {col_with_s3s} FROM {flat_repositories} WHERE namespace='{country}'\")\n",
    "    csv_res = df_s3s.select(col_with_s3s).distinct().collect()\n",
    "    csv_set = set()\n",
    "    for i in range(len(csv_res)):\n",
    "        s3s_in_record = csv_res[i][0]\n",
    "        s3s_csvs = []\n",
    "        for item in s3s_in_record:\n",
    "            if (db != \"\"):\n",
    "                if (ntpath.dirname(item).split('/')[1] != db): continue\n",
    "            if (item.endswith(\".csv\")): \n",
    "                item = f\"{data_bucket}/{item}\"\n",
    "                file_existance, debug_output = file_exists(item)\n",
    "                if ( file_existance == True):\n",
    "                    s3s_csvs.append(item)\n",
    "                else:\n",
    "                    print(f\"\\n*** Warning: File '{item}' doesn't exist; skipping it ***\\n\")\n",
    "        csv_set.update(s3s_csvs)\n",
    "    return csv_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "144ff84e-18ac-4f75-b605-357692c26bd0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Pre-process csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48dfd305-7a99-47c6-9ab3-863b101434d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def pre_process_big_csv(csv_filepath, delimiter, catalog_bucket):\n",
    "    \"\"\"Create only the schema of the table, based on the first part of the file\"\"\"\n",
    "    filename = ntpath.basename(csv_filepath)\n",
    "    print(f\"\\n'{filename}' is bigger than 10M, and needs its header adjusted\")\n",
    "\n",
    "    content = dbutils.fs.head(csv_filepath) # Read the first part of the csv generate the schema from it\n",
    "    split_content = content.split(\"\\n\")\n",
    "    header = split_content[0] # Takes the first element as the header\n",
    "    content = '\\n'.join(split_content[1:-1]) # main content without heade and last line (as its probably incomplete)\n",
    "    non_alphn = get_non_alphanumeric_chars(header)\n",
    "\n",
    "    # Replaces carriage returns and special characters from the header\n",
    "    header = header.replace(\"\\r\", \"_\")\n",
    "    for char in non_alphn:\n",
    "        header = header.replace(char, \"_\")\n",
    "    header = re.sub('[^0-9a-zA-Z,;\\t|]+', '_', header)\n",
    "    \n",
    "    # Replace the header with an adjusted version\n",
    "    new_content = header.replace(\" \", \"_\") + \"\\n\" + content.replace(\"\\r\", \" \")\n",
    "\n",
    "    new_csv_filename = ntpath.basename(csv_filepath).removesuffix(\".csv\") # Get the filename\n",
    "\n",
    "    # Generate the temporary csv file\n",
    "    new_csv_filepath = create_temp_csv(new_content, new_csv_filename, catalog_bucket)     \n",
    "\n",
    "    temp_df = spark.read.format(\"csv\") \\\n",
    "      .option(\"inferSchema\", \"true\") \\\n",
    "      .option(\"sep\", delimiter) \\\n",
    "      .option(\"header\", \"true\") \\\n",
    "      .option(\"ignoreLeadingWhiteSpace\", \"true\") \\\n",
    "      .option(\"ignoreTrailingWhiteSpace\", \"true\") \\\n",
    "      .option(\"multiLine\", \"true\") \\\n",
    "      .option(\"nullvalue\", \"NA\") \\\n",
    "      .load(new_csv_filepath)\n",
    "    \n",
    "    # Gene an empty df with only the schema\n",
    "    schema_df = spark.createDataFrame(data = [], schema=temp_df.schema)\n",
    "\n",
    "    catalog, db_name = unity_path_from_filename(csv_filepath)\n",
    "\n",
    "    # Assumes the catalog and db have already been created\n",
    "    table_name = generate_table_name(filename)\n",
    "    table_path = f\"{catalog}.{db_name}.{table_name}\"\n",
    "    schema_df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"delta.minReaderVersion\", 1) \\\n",
    "        .option(\"delta.minWriterVersion\", 4) \\\n",
    "        .saveAsTable(table_path)\n",
    "    return temp_df.schema\n",
    "\n",
    "def pre_process_csv(csv_filepath):\n",
    "    \"\"\"Load the first part of a csv file to identify the delimiter and if there's a need to adjust the header\n",
    "    Returns a tuple with the delimiter and a path to a temporary csv in case it needs to adjust it.\n",
    "    In particular, if there are carriage returns (^M) within the column names in the header it removes them,\n",
    "    as they would cause the header to be partially processed and generate inconsistencies\n",
    "    \"\"\"\n",
    "    filename = ntpath.basename(csv_filepath)\n",
    "    print(f\"\\nTruncating the read of '{filename}' to just review its header\")\n",
    "    content = dbutils.fs.head(csv_filepath, 2000) # Read the first 2Kb of the csv to review its header\n",
    "    return_pos = content.find(\"\\n\") # Needed if there's a header replacement\n",
    "    header = content.split(\"\\n\")[0] # Takes header as the content up to a newline\n",
    "    delimiter = find_delimiter(header)\n",
    "\n",
    "    # If the header has carriage return or unalowed character, replace it for a '_' \n",
    "    # and write/load csv from a temp file\n",
    "    new_csv_filepath = \"\"\n",
    "    find_pos = header.find(\"\\r\")\n",
    "    non_alphn = get_non_alphanumeric_chars(header)\n",
    "\n",
    "    # If there is a carriage return or special character in the middle of the header\n",
    "    if ((find_pos > 0) and (find_pos < (len(header)-2)) or (len(non_alphn)>0) ):\n",
    "        # If the file is bigger than 10M, return the same filepath for processing it differently\n",
    "        file_info = dbutils.fs.ls(csv_filepath)[0]\n",
    "        if (file_info.size > 10000000):\n",
    "            new_csv_filepath = csv_filepath\n",
    "            return delimiter, new_csv_filepath\n",
    "\n",
    "        # Replaces carriage returns and special characters from the header\n",
    "        header = header.replace(\"\\r\", \"_\")\n",
    "        for char in non_alphn:\n",
    "            header = header.replace(char, \"_\")\n",
    "        \n",
    "        # Read the whole csv as plain text, and replace the header with an adjusted version\n",
    "        df = spark.read.text(csv_filepath, wholetext=True)\n",
    "        content = df.first()['value']\n",
    "        new_content = header.replace(\" \", \"_\") + content[return_pos:].replace(\"\\r\", \" \")\n",
    "        new_csv_filename = ntpath.basename(csv_filepath).removesuffix(\".csv\") # Get the filename\n",
    "\n",
    "        # Generate the temporary csv file\n",
    "        new_csv_filepath = create_temp_csv(new_content, new_csv_filename, catalog_bucket) \n",
    "        \n",
    "    return delimiter, new_csv_filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "118196ad-1847-446f-a086-f0c605162037",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create delta table from csv function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d099f218-11f0-4b91-a4ca-01d91156768d",
     "showTitle": false,
     "title": "Create delta table from csv function"
    }
   },
   "outputs": [],
   "source": [
    "def create_table_from_csv(source_filepath, catalog=\"\", db=\"\", catalog_bucket=\"\",\n",
    "        tmp_csv_filepath=\"\",\n",
    "        table_name=\"\",\n",
    "        inferschema=\"true\",\n",
    "        header=\"true\",\n",
    "        delimiter=\";\",\n",
    "        encoding=\"UTF-8\",\n",
    "        ignoreLeadingWhiteSpace=\"true\",\n",
    "        ignoreTrailingWhiteSpace=\"true\",\n",
    "        multiLine=\"true\",\n",
    "        nullvalue=\"NA\",\n",
    "        write_mode = \"overwrite\",\n",
    "        reader_version = 1,\n",
    "        writer_version = 4,\n",
    "        description = \"\"):\n",
    "    \"\"\"Create a delta table from a csv file. Includes common csv and delta table options\"\"\"\n",
    "\n",
    "    if (catalog == \"\"): catalog, db = unity_path_from_filename(source_filepath)\n",
    "    # If the catalog or db don't exist yet, create them\n",
    "    create_catalog(catalog, catalog_bucket)\n",
    "    create_db(db, catalog, catalog_bucket)\n",
    "    badRecordsPath = f\"{catalog_bucket}/badRecords/{catalog}/{db}\"\n",
    "    \n",
    "    # If it needs to read from a temporary pre-processed csv, it specifies it as input     \n",
    "    if (tmp_csv_filepath != \"\"):\n",
    "        # If the temp csv is the same, it means that it will process a big csv\n",
    "        # IMPROVE THIS LOGIC / solution (its a bit cryptic)..\n",
    "        if (source_filepath == tmp_csv_filepath):\n",
    "            inferschema = \"false\" # The schema will already be specified\n",
    "            schema = pre_process_big_csv(source_filepath, delimiter, catalog_bucket)\n",
    "            header = \"false\" # the first record with the header will probably be a bad record\n",
    "            write_mode = \"append\"\n",
    "        input_file_path = tmp_csv_filepath\n",
    "    else: \n",
    "        input_file_path = source_filepath\n",
    "\n",
    "    # If not infering schema, it means that we're specifying it by hand\n",
    "    if (inferschema == \"false\"):\n",
    "        df = spark.read.format(\"csv\") \\\n",
    "        .option(\"inferSchema\", inferschema) \\\n",
    "        .option(\"sep\", delimiter) \\\n",
    "        .option(\"header\", header) \\\n",
    "        .option(\"encoding\", encoding) \\\n",
    "        .option(\"ignoreLeadingWhiteSpace\", ignoreLeadingWhiteSpace) \\\n",
    "        .option(\"ignoreTrailingWhiteSpace\", ignoreTrailingWhiteSpace) \\\n",
    "        .option(\"multiLine\", multiLine) \\\n",
    "        .option(\"nullvalue\", nullvalue) \\\n",
    "        .option(\"badRecordsPath\", badRecordsPath) \\\n",
    "        .schema(schema) \\\n",
    "        .load(input_file_path)        \n",
    "    else:\n",
    "        df = spark.read.format(\"csv\") \\\n",
    "        .option(\"inferSchema\", inferschema) \\\n",
    "        .option(\"sep\", delimiter) \\\n",
    "        .option(\"header\", header) \\\n",
    "        .option(\"encoding\", encoding) \\\n",
    "        .option(\"ignoreLeadingWhiteSpace\", ignoreLeadingWhiteSpace) \\\n",
    "        .option(\"ignoreTrailingWhiteSpace\", ignoreTrailingWhiteSpace) \\\n",
    "        .option(\"multiLine\", multiLine) \\\n",
    "        .option(\"nullvalue\", nullvalue) \\\n",
    "        .option(\"badRecordsPath\", badRecordsPath) \\\n",
    "        .load(input_file_path)\n",
    "\n",
    "    # Adding a description for the table\n",
    "    creation_note = \"Table schema and data created automatically from \" + source_filepath\n",
    "    if (description != \"\"): description = creation_note + \" . \" + description\n",
    "    else: description = creation_note\n",
    "    \n",
    "    # Replace special characters and white spaces in column names\n",
    "    df = df.select([col(c).alias(re.sub('[^0-9a-zA-Z]+', '_', c)) for c in df.columns])\n",
    "    df = df.select([col(c).alias(c.replace(\" \", \"_\")) for c in df.columns])\n",
    "\n",
    "    # If the table name is not specified, created based on the csv filename\n",
    "    if (table_name == \"\"):\n",
    "        file_name = ntpath.basename(source_filepath)\n",
    "        # adds prefix and removes .csv suffix and special characters\n",
    "        table_name = generate_table_name(file_name)\n",
    "\n",
    "    # When writing, the option key-value pairs will be saved as metadata\n",
    "    table_path = f\"{catalog}.{db}.{table_name}\"\n",
    "    df.write.format(\"delta\") \\\n",
    "        .mode(write_mode) \\\n",
    "        .option(\"delta.minReaderVersion\", reader_version) \\\n",
    "        .option(\"delta.minWriterVersion\", writer_version) \\\n",
    "        .saveAsTable(table_path)\n",
    "\n",
    "    spark.sql(f\"COMMENT ON TABLE {table_path} IS \\\"{description}\\\"\")\n",
    "    spark.sql(f\"ALTER TABLE {table_path} SET TBLPROPERTIES ('source_csv_file'='{source_filepath}', \\\n",
    "        'write_mode'='{write_mode}', 'source_delimiter'='{delimiter}')\")\n",
    "    print (f\"{table_path} table saved\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88bbd39c-2976-4bc6-b30f-f3c13db33613",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# World tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04a4d736-7da6-4929-a728-08fba31853d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "country = \"world\"\n",
    "csv_set = list_countries_csvs(country, flat_repositories, col_with_s3s, data_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d500bc7e-dbcf-4057-bda3-6bdb1fd7eaea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9be174b9-6796-4bc3-adfe-7e7189a0962b",
     "showTitle": true,
     "title": "Actual creation of tables"
    }
   },
   "outputs": [],
   "source": [
    "for csv_file in csv_set:\n",
    "    delimiter, temp_csv_path = pre_process_csv(csv_file)\n",
    "    create_table_from_csv(source_filepath=csv_file, tmp_csv_filepath=temp_csv_path, delimiter=delimiter, catalog_bucket=catalog_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "201d2657-d619-4997-820f-b61632ff7df9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Indonesia tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf174d5c-8367-4e70-aafb-e6cf37d500b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "country = \"indonesia\"\n",
    "csv_set = list_countries_csvs(country, flat_repositories, col_with_s3s, data_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3343995f-194e-4e6c-a5fc-ed38ae4ff85e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1889f94c-f661-4ce6-a907-61a435d76191",
     "showTitle": true,
     "title": "Actual creation of tables"
    }
   },
   "outputs": [],
   "source": [
    "for csv_file in csv_set:\n",
    "    delimiter, temp_csv_path = pre_process_csv(csv_file)\n",
    "    create_table_from_csv(source_filepath=csv_file, tmp_csv_filepath=temp_csv_path, delimiter=delimiter, catalog_bucket=catalog_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "299e3bca-ddc8-42de-a57d-f812198dba92",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Brazil tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58c36e81-1a22-4881-9a0d-a5652b676b56",
     "showTitle": true,
     "title": "Basic check of the folder substructure"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "LIST \"s3a://****/brazil/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f98c1e95-56a7-4778-aa2c-acf9e14b4b0a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Brazil soy tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a404cff0-a4cc-4ebe-a107-cb2f7db61589",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "country = \"brazil\"\n",
    "db = \"soy\"\n",
    "csv_set = list_countries_csvs(country, flat_repositories, col_with_s3s, data_bucket, db=db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67de76e5-9c16-46cc-8165-d96f9298431f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "178b4bde-45c6-46a0-9d5b-dd8f4678f97c",
     "showTitle": true,
     "title": "Actual creation of tables"
    }
   },
   "outputs": [],
   "source": [
    "for csv_file in csv_set:\n",
    "    delimiter, temp_csv_path = pre_process_csv(csv_file)\n",
    "    create_table_from_csv(source_filepath=csv_file, tmp_csv_filepath=temp_csv_path, delimiter=delimiter, catalog_bucket=catalog_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57da67f3-c1f7-4d9e-bb3d-7c2016d17119",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Brazil auxiliary tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "766b4f92-b296-4b10-91c9-544d56e29c15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "country = \"brazil\"\n",
    "db = \"auxiliary\"\n",
    "csv_set = list_countries_csvs(country, flat_repositories, col_with_s3s, data_bucket, db=db)\n",
    "csv_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "013947ed-eddd-44bd-8db9-9c92d82a1007",
     "showTitle": true,
     "title": "Actual creation of tables"
    }
   },
   "outputs": [],
   "source": [
    "for csv_file in csv_set:\n",
    "    delimiter, temp_csv_path = pre_process_csv(csv_file)\n",
    "    create_table_from_csv(source_filepath=csv_file, tmp_csv_filepath=temp_csv_path, delimiter=delimiter, catalog_bucket=catalog_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8061b60-8535-4fbd-b890-feda6689381d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Brazil dictionaries tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd530997-ff51-45b1-b89f-8f6e430182c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "country = \"brazil\"\n",
    "db = \"dictionaries\"\n",
    "csv_set = list_countries_csvs(country, flat_repositories, col_with_s3s, data_bucket, db=db)\n",
    "csv_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d3a98a8-8c44-4d86-947f-6e01c3f53eab",
     "showTitle": true,
     "title": "Actual creation of tables"
    }
   },
   "outputs": [],
   "source": [
    "for csv_file in csv_set:\n",
    "    delimiter, temp_csv_path = pre_process_csv(csv_file)\n",
    "    create_table_from_csv(source_filepath=csv_file, tmp_csv_filepath=temp_csv_path, delimiter=delimiter, catalog_bucket=catalog_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32e815a5-6d69-46f2-a6b5-fd18825bcd3f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Brazil flow constraints tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e99ed125-cb6e-4065-9676-bcc9b9297815",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "country = \"brazil\"\n",
    "db = \"flow_constraints\"\n",
    "csv_set = list_countries_csvs(country, flat_repositories, col_with_s3s, data_bucket, db=db)\n",
    "csv_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07e1f059-56c2-414d-8aee-80bf2ddb737f",
     "showTitle": true,
     "title": "Actual creation of tables"
    }
   },
   "outputs": [],
   "source": [
    "for csv_file in csv_set:\n",
    "    delimiter, temp_csv_path = pre_process_csv(csv_file)\n",
    "    create_table_from_csv(source_filepath=csv_file, tmp_csv_filepath=temp_csv_path, delimiter=delimiter, catalog_bucket=catalog_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae674dcd-29b2-47d4-bab5-d03ea9b917c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Brazil indicators tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "981a34f9-7acf-4dc7-89ce-32dc5ff974a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "country = \"brazil\"\n",
    "db = \"indicators\"\n",
    "csv_set = list_countries_csvs(country, flat_repositories, col_with_s3s, data_bucket, db=db)\n",
    "csv_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "369e3c60-6dd7-45cc-9c98-d94ae67b80e3",
     "showTitle": true,
     "title": "Actual creation of tables"
    }
   },
   "outputs": [],
   "source": [
    "for csv_file in csv_set:\n",
    "    delimiter, temp_csv_path = pre_process_csv(csv_file)\n",
    "    create_table_from_csv(source_filepath=csv_file, tmp_csv_filepath=temp_csv_path, delimiter=delimiter, catalog_bucket=catalog_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3311a4c-d68d-4035-8caa-9465b5e329d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Brazil logistics tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5d8ecc5-5e27-4e94-99dc-8ca961b40755",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "country = \"brazil\"\n",
    "db = \"logistics\"\n",
    "csv_set = list_countries_csvs(country, flat_repositories, col_with_s3s, data_bucket, db=db)\n",
    "csv_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2e516a9-e472-413f-ab0b-7ecd9810a062",
     "showTitle": true,
     "title": "Actual creation of tables"
    }
   },
   "outputs": [],
   "source": [
    "for csv_file in csv_set:\n",
    "    delimiter, temp_csv_path = pre_process_csv(csv_file)\n",
    "    create_table_from_csv(source_filepath=csv_file, tmp_csv_filepath=temp_csv_path, delimiter=delimiter, catalog_bucket=catalog_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "499b4325-8d13-497b-a8e2-9c4f94995cf8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Brazil metadata tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a60c484-0a1e-4410-9c53-e68f131019f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "country = \"brazil\"\n",
    "db = \"metadata\"\n",
    "csv_set = list_countries_csvs(country, flat_repositories, col_with_s3s, data_bucket, db=db)\n",
    "csv_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23327e8b-6806-4c82-ab9d-696927058a46",
     "showTitle": true,
     "title": "Actual creation of tables"
    }
   },
   "outputs": [],
   "source": [
    "for csv_file in csv_set:\n",
    "    delimiter, temp_csv_path = pre_process_csv(csv_file)\n",
    "    create_table_from_csv(source_filepath=csv_file, tmp_csv_filepath=temp_csv_path, delimiter=delimiter, catalog_bucket=catalog_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3b82a1f-f95b-489e-b4e5-2876293d0536",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Brazil production tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a26c3a6-e1e0-47b0-92ef-8ac639e23282",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "country = \"brazil\"\n",
    "db = \"production\"\n",
    "csv_set = list_countries_csvs(country, flat_repositories, col_with_s3s, data_bucket, db=db)\n",
    "csv_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7598c35f-fe0e-473c-905c-967a5b71cced",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Brazil risk_threshold tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b73d05ea-6d1f-446e-ad75-806e786a9d5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "country = \"brazil\"\n",
    "db = \"risk_threshold\"\n",
    "csv_set = list_countries_csvs(country, flat_repositories, col_with_s3s, data_bucket, db=db)\n",
    "csv_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18439f46-cc4d-4de0-897d-48aa18c5c82b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Brazil sei_pcs tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a542587-a451-46b2-9435-4739abc2afb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "country = \"brazil\"\n",
    "db = \"sei_pcs\"\n",
    "csv_set = list_countries_csvs(country, flat_repositories, col_with_s3s, data_bucket, db=db)\n",
    "csv_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "768b12a4-0204-4113-aae4-e5849f3c2102",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Brazil spatial tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29627238-8ca1-4170-a8ab-9c07d76feb6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "country = \"brazil\"\n",
    "db = \"spatial\"\n",
    "csv_set = list_countries_csvs(country, flat_repositories, col_with_s3s, data_bucket, db=db)\n",
    "csv_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a84ec0f7-7895-4f80-bb11-45f277dbecb4",
     "showTitle": true,
     "title": "Actual creation of tables"
    }
   },
   "outputs": [],
   "source": [
    "for csv_file in csv_set:\n",
    "    delimiter, temp_csv_path = pre_process_csv(csv_file)\n",
    "    create_table_from_csv(source_filepath=csv_file, tmp_csv_filepath=temp_csv_path, delimiter=delimiter, catalog_bucket=catalog_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c217d58d-9258-4827-a45c-e17a1a33d484",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Brazil trade tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "282c0317-45fc-4f22-94a8-e418c8ab6da3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "country = \"brazil\"\n",
    "db = \"trade\"\n",
    "csv_set = list_countries_csvs(country, flat_repositories, col_with_s3s, data_bucket, db=db)\n",
    "csv_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "defa4864-bab9-4052-a0fc-386897ab97f1",
     "showTitle": true,
     "title": "Actual creation of tables"
    }
   },
   "outputs": [],
   "source": [
    "for csv_file in csv_set:\n",
    "    delimiter, temp_csv_path = pre_process_csv(csv_file)\n",
    "    create_table_from_csv(source_filepath=csv_file, tmp_csv_filepath=temp_csv_path, delimiter=delimiter, catalog_bucket=catalog_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3095896-a2af-4fc0-a14b-faac406ec9ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Brazil CNAE secondary\n",
    "As `CNPJ_2019_CNAE_secondary.csv` is not within the requirements.yml file, we load it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2831344e-34c4-47aa-a55a-0579fc29ad3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cnae_secondary_path = \"s3://****/brazil/auxiliary/cnpj/original/CNPJ_2019_CNAE_secondary.csv\"\n",
    "delimiter, temp_csv_path = pre_process_csv(cnae_secondary_path)\n",
    "create_table_from_csv(source_filepath=cnae_secondary_path, tmp_csv_filepath=temp_csv_path, delimiter=delimiter, catalog_bucket=catalog_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5e92b6c-8a48-44dc-9edc-1bed99eaf769",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# From here on, debugging cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a41714e3-25f5-41cf-90cc-8910b7fcdbf3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "LIST 's3a://****/brazil/auxiliary/cnpj/original'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e17ffab-2602-4f85-9b15-1bdeabdc6711",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cnae_secondary_path = \"s3://****/brazil/auxiliary/cnpj/original/CNPJ_2019_CNAE_secondary.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06352b8c-ab6d-40f5-8315-c58ee18930c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_path=\"s3a://****/brazil/soy/auxiliary/cnpj/SOY_CNPJ_2016.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51bee5c4-140a-4ba8-adda-3ce0dfcf832d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "delimiter, temp_csv_path = pre_process_csv(test_path)\n",
    "create_table_from_csv(source_filepath=test_path, tmp_csv_filepath=temp_csv_path, delimiter=delimiter, catalog_bucket=catalog_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5619bf46-7891-4609-b502-a38f74be2f25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "LIST 's3a://****/badRecords/brazil/auxiliary/20230126T171755/bad_records/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bad6d6fa-280a-47ca-b003-b9f0264db6b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.head(\"s3a://****/brazil/indicators/out/br_legal_reserve_deficit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dd5e4ab-9cc5-4ade-b895-33de474de522",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "LIST \"s3a://****/brazil/sei_pcs/qa_ed/post-processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf524d80-2a23-479b-ac6c-3f9a6c2161db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "create_tables_from_repositories_catalog",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
