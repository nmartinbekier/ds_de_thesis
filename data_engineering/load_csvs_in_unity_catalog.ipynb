{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96816ed7-b08a-4ca3-abb9-f99880c3535d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook uploads csvs to Delta Tables in Unity Catalog.\n",
    "\n",
    "It requires:\n",
    "* The folder path where it will recursively process csvs from\n",
    "* The catalog and database where the tables will be uploaded\n",
    "\n",
    "Notice:\n",
    "* The tables will have the same name as the files, with a `bronze_` prefix, and replacing dashes and parenthesis for underscores\n",
    "* If there are failed records, they will be placed in the specified badRecordPath for the table\n",
    "* The tables will include as metadata the original filepath, delimiter used, badRecordPath\n",
    "* Each csv is initially loaded as a raw text file to identify its delimiter, and to check if it has carriage returns (^M). If it does, it removes them from the header and creates an adjusted temporary csv that will be used to create the table from.\n",
    "\n",
    "To-do:\n",
    "* Create a table registering the details of each transaction, including:\n",
    "  * Timestamp with the start of the process\n",
    "  * Filepath of csv source\n",
    "  * Filename of csv source\n",
    "  * Name of the table, database and catalog (in different fields)\n",
    "  * Success of the transaction (bool)\n",
    "  * Number of records written\n",
    "  * Number of bad records\n",
    "  * badRecordPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e3197de-c2f9-4df8-a63f-cfeef568c6e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Global definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71e8c28a-6799-4e43-afd3-81351f905d31",
     "showTitle": false,
     "title": "Global definitions"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "import re\n",
    "import ntpath\n",
    "data_bucket = \"s3a://trase-storage\"\n",
    "catalog_bucket = \"s3a://trase-catalog\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9af2766e-529f-4b58-9868-53ad65e3545e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Main functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2881271f-9205-4d81-87a1-f8969ebeacf9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## General supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "520b1a39-73f5-4e57-b637-d1f5d0c45e73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def find_delimiter(header_text):\n",
    "    \"\"\"Given a header, returns the first ocurrence of a delimiter character from |;,\\t \"\"\"\n",
    "    delimiters = [\";\",\",\",\"\\t\",\"|\"]\n",
    "    del_index = next((i for i,c in enumerate(header_text) if c in delimiters))\n",
    "    return(header_text[del_index])\n",
    "\n",
    "def get_non_alphanumeric_chars(string):\n",
    "    non_alphanumeric_chars = set(re.findall(r'[^a-zA-Z0-9]', string))\n",
    "    allowed_subet = set([\";\",\",\",\"\\t\",\"|\",\" \",\"_\",\"\\r\",\"\\n\"])\n",
    "    return list(non_alphanumeric_chars - allowed_subet)\n",
    "\n",
    "def create_temp_csv(new_content, file_name, data_bucket):\n",
    "    \"\"\"Creates a temporary csv based on the string provided and places it in s3:{data_bucket}/tmp/file_name_TSTIMESTAMP.csv\"\"\"\n",
    "    current_time = int(time.time())\n",
    "    filepath = f\"{data_bucket}/tmp/{file_name}_TS{current_time}.csv\"\n",
    "    dbutils.fs.put(filepath, new_content)\n",
    "    return(filepath)\n",
    "\n",
    "def dir_contents(path):\n",
    "    \"\"\"Returns a list of filenames and a list of directories within file_path\"\"\"\n",
    "    records_list = dbutils.fs.ls(f\"{path}\")\n",
    "    file_list = []\n",
    "    dir_list = []\n",
    "    for record in records_list:\n",
    "        if (record.isFile() == True): file_list.append(record.name)\n",
    "        elif (record.isDir() == True): dir_list.append(record.name)\n",
    "    return(file_list, dir_list)\n",
    "\n",
    "def list_files_recursively(dir_path, files_list):\n",
    "    \"\"\"Recursively list files of a directory and return them in provided 'files_list'\"\"\" \n",
    "    for record in dbutils.fs.ls(dir_path):\n",
    "        file_path = f\"{dir_path}/{record.name}\"\n",
    "        if (record.isDir() == True):\n",
    "            list_files_recursively(file_path[:-1], files_list)\n",
    "        else:\n",
    "            if (record.size > 50000000):\n",
    "                print(f\"\\n***\\nWarning: {file_path} is bigger than 50M, skipping it. \\\n",
    "                    Header replacement has to be done manually for the moment\\n***\")\n",
    "                break;\n",
    "            elif (record.name.lower().find(\".csv\") == -1):\n",
    "                print(f\"***Info: {file_path} is not a csv file. Skipping it***\")\n",
    "            else:\n",
    "                files_list.append(file_path)\n",
    "\n",
    "def generate_table_name(csv_file_name):\n",
    "    \"\"\"Returns a table name with a 'bronze_' prefix, and replacing special characters\"\"\"\n",
    "    table_name = re.sub('[^0-9a-zA-Z]+', '_', csv_file_name).lower().removesuffix(\"_csv\")\n",
    "    return(\"bronze_\"+table_name)\n",
    "\n",
    "def create_catalog(catalog_name, catalog_bucket, comment=\"\"):\n",
    "    location = f\"{catalog_bucket}/{catalog_name}\"\n",
    "    comment = f\"{catalog_name} catalog, hosted in {location}. {comment}\"\n",
    "    spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name} \" \\\n",
    "    f\" MANAGED LOCATION '{location}' \" \\\n",
    "    f\" COMMENT '{comment}' \")\n",
    "    return True\n",
    "\n",
    "def create_db(db_name, catalog_name, catalog_bucket, comment=\"\"):\n",
    "    location = f\"{catalog_bucket}/{catalog_name}/{db_name}\"\n",
    "    comment = f\"{db_name} database, hosted in {location}\"\n",
    "    spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name} \" \\\n",
    "    f\" MANAGED LOCATION '{location}' \" \\\n",
    "    f\" COMMENT '{comment}' \")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7867e294-fe01-47e0-b54e-98acf4c9a366",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Pre-process csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d3505e9-5400-4893-ae8d-ec14289a1c85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def pre_process_csv(csv_filepath):\n",
    "    \"\"\"Load the first part of a csv file to identify the delimiter and if there's a need to adjust the header\n",
    "    Returns a tuple with the delimiter and a path to a temporary csv in case it needs to adjust it.\n",
    "    In particular, if there are carriage returns (^M) within the column names in the header it removes them,\n",
    "    as they would cause the header to be partially processed and generate inconsistencies\n",
    "    \"\"\"\n",
    "    filename = ntpath.basename(csv_filepath)\n",
    "    print(f\"\\nTruncating the read of '{filename}' to just review its header\")\n",
    "    content = dbutils.fs.head(csv_filepath, 2000) # Read the first 2Kb of the csv to review its header\n",
    "    return_pos = content.find(\"\\n\") # Needed if there's a header replacement\n",
    "    header = content.split(\"\\n\")[0] # Takes header as the content up to a newline\n",
    "    delimiter = find_delimiter(header)\n",
    "\n",
    "    # If the header has carriage return or unalowed character, replace it for a '_' \n",
    "    # and write/load csv from a temp file\n",
    "    new_csv_filepath = \"\"\n",
    "    find_pos = header.find(\"\\r\")\n",
    "    non_alphn = get_non_alphanumeric_chars(header)\n",
    "    # If there is a carriage return or special character in the middle of the header\n",
    "    if ((find_pos > 0) and (find_pos < (len(header)-2)) or (len(non_alphn)>0) ):\n",
    "        # Replaces carriage returns and special characters from the header\n",
    "        header = header.replace(\"\\r\", \"_\")\n",
    "        for char in non_alphn:\n",
    "            header = header.replace(char, \"_\")\n",
    "        \n",
    "        # Read the whole csv as plain text, and replace the header with an adjusted version\n",
    "        df = spark.read.text(csv_filepath, wholetext=True)\n",
    "        content = df.first()['value']\n",
    "        new_content = header.replace(\" \", \"_\") + content[return_pos:].replace(\"\\r\", \" \")\n",
    "        new_csv_filename = ntpath.basename(csv_filepath).removesuffix(\".csv\") # Get the filename\n",
    "\n",
    "        # Generate the temporary csv file\n",
    "        new_csv_filepath = create_temp_csv(new_content, new_csv_filename, data_bucket) \n",
    "        \n",
    "    return delimiter, new_csv_filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5336b475-88bf-43d1-98b1-01b76113540d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create delta table from csv function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dea790d3-9209-4c32-94d3-44543c766ea6",
     "showTitle": false,
     "title": "Create delta table from csv function"
    }
   },
   "outputs": [],
   "source": [
    "def create_table_from_csv(source_filepath, catalog, db, \n",
    "        tmp_csv_filepath=\"\",\n",
    "        table_name=\"\",\n",
    "        inferschema=\"true\",\n",
    "        header=\"true\",\n",
    "        delimiter=\";\",\n",
    "        encoding=\"UTF-8\",\n",
    "        ignoreLeadingWhiteSpace=\"true\",\n",
    "        ignoreTrailingWhiteSpace=\"true\",\n",
    "        multiLine=\"true\",\n",
    "        nullvalue=\"NA\",\n",
    "        badRecordsPath=f\"{data_bucket}/tmp/badRecordsPath\",\n",
    "        write_mode = \"overwrite\",\n",
    "        reader_version = 1,\n",
    "        writer_version = 4,\n",
    "        description = \"\"):\n",
    "    \"\"\"Create a delta table from a csv file. Includes common csv and delta table options\"\"\"\n",
    "\n",
    "    # If it needs to read from a temporary pre-processed csv, it specifies it as input    \n",
    "    if (tmp_csv_filepath != \"\"):\n",
    "        input_file_path = tmp_csv_filepath\n",
    "    else: \n",
    "        input_file_path = source_filepath\n",
    "\n",
    "    df = spark.read.format(\"csv\") \\\n",
    "      .option(\"inferSchema\", inferschema) \\\n",
    "      .option(\"sep\", delimiter) \\\n",
    "      .option(\"header\", header) \\\n",
    "      .option(\"encoding\", encoding) \\\n",
    "      .option(\"ignoreLeadingWhiteSpace\", ignoreLeadingWhiteSpace) \\\n",
    "      .option(\"ignoreTrailingWhiteSpace\", ignoreTrailingWhiteSpace) \\\n",
    "      .option(\"multiLine\", multiLine) \\\n",
    "      .option(\"nullvalue\", nullvalue) \\\n",
    "      .option(\"badRecordsPath\", badRecordsPath) \\\n",
    "      .load(input_file_path)\n",
    "\n",
    "    # Adding a description for the table\n",
    "    creation_note = \"Table schema and data created automatically from \" + source_filepath\n",
    "    if (description != \"\"): description = creation_note + \" . \" + description\n",
    "    else: description = creation_note\n",
    "    \n",
    "    # Replace white spaces in column names\n",
    "    #df = df.select([col(c).alias(re.sub('[^0-9a-zA-Z]+', '_', c)) for c in df.columns])\n",
    "    df = df.select([col(c).alias(c.replace(\" \", \"_\")) for c in df.columns])\n",
    "\n",
    "    # If the table name is not specified, created based on the csv filename\n",
    "    if (table_name == \"\"):\n",
    "        file_name = ntpath.basename(source_filepath)\n",
    "        # adds prefix and removes .csv suffix and special characters\n",
    "        table_name = generate_table_name(file_name)\n",
    "\n",
    "    # When writing, the option key-value pairs will be saved as metadata\n",
    "    table_path = f\"{catalog}.{db}.{table_name}\"\n",
    "    df.write.format(\"delta\") \\\n",
    "        .mode(write_mode) \\\n",
    "        .option(\"delta.minReaderVersion\", reader_version) \\\n",
    "        .option(\"delta.minWriterVersion\", writer_version) \\\n",
    "        .saveAsTable(table_path)\n",
    "\n",
    "    spark.sql(f\"COMMENT ON TABLE {table_path} IS \\\"{description}\\\"\")\n",
    "    spark.sql(f\"ALTER TABLE {table_path} SET TBLPROPERTIES ('source_csv_file'='{source_filepath}', \\\n",
    "        'write_mode'='{write_mode}', 'source_delimiter'='{delimiter}')\")\n",
    "    print (f\"{table_path} table saved\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ada096d2-404b-4528-96a7-184d7cd62933",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Creation of Ecuador tables\n",
    "From here on, do the actual loading. Making an example with the Ecuador tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94f53530-c589-4dfc-99b1-779177aeffb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[64]: True"
     ]
    }
   ],
   "source": [
    "catalog = \"ecuador\"\n",
    "create_catalog(catalog_name=catalog, catalog_bucket=catalog_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de2448dc-0f65-40fa-967a-cb020822b33a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Ecuador logistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d787aba-389d-4b7c-b652-d7e848666216",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "db = \"logistics\"\n",
    "create_db(db_name=db, catalog_name=catalog, catalog_bucket=catalog_bucket)\n",
    "dir_path = f\"{data-bucket}/ecuador/logistics\"\n",
    "files_list = []\n",
    "list_files_recursively(dir_path, files_list)\n",
    "print(files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33f96f7b-a8d6-4359-9030-82a385f5d6e2",
     "showTitle": true,
     "title": "Actual creation"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTruncating the read of 'certification_data_cleaned.csv' to just review its header\n[Truncated to first 1000 bytes]\necuador_catalog.logistics.bronze_certification_data_cleaned table saved\n\nTruncating the read of 'company_links_cleaned.csv' to just review its header\n[Truncated to first 1000 bytes]\necuador_catalog.logistics.bronze_company_links_cleaned table saved\n\nTruncating the read of 'traders.csv' to just review its header\n[Truncated to first 1000 bytes]\necuador_catalog.logistics.bronze_traders table saved\n\nTruncating the read of 'ACTUALIZA_PROCESA_ACUACULTURA_12OCTUBRE2017.csv' to just review its header\n[Truncated to first 1000 bytes]\necuador_catalog.logistics.bronze_actualiza_procesa_acuacultura_12octubre2017 table saved\n\nTruncating the read of 'ACTUALIZA_PROCESA_PESCA_ACUA_22DICIEMBRE2017.csv' to just review its header\n[Truncated to first 1000 bytes]\necuador_catalog.logistics.bronze_actualiza_procesa_pesca_acua_22diciembre2017 table saved\n\nTruncating the read of 'tabula-ACTUALIZA_PROCESA_ACUACULTURA_31AGOSTO2018.csv' to just review its header\n[Truncated to first 1000 bytes]\nWrote 7026 bytes.\necuador_catalog.logistics.bronze_tabula_actualiza_procesa_acuacultura_31agosto2018 table saved\n\nTruncating the read of 'tabula-ACTUALIZA_PROCESA_ACUACULTURA_31AGOSTO2018_wout_carr.csv' to just review its header\n[Truncated to first 1000 bytes]\necuador_catalog.logistics.bronze_tabula_actualiza_procesa_acuacultura_31agosto2018_wout_carr table saved\n\nTruncating the read of 'tabula-ACTUALIZA_PROCESA_PESCA_ACUA_13AGOSTO2018.csv' to just review its header\n[Truncated to first 1000 bytes]\nWrote 5923 bytes.\necuador_catalog.logistics.bronze_tabula_actualiza_procesa_pesca_acua_13agosto2018 table saved\n\nTruncating the read of 'tabula-ACTUALIZA_PROCESA_PESCA_ACUA_13AGOSTO2018_wout_carr.csv' to just review its header\n[Truncated to first 1000 bytes]\necuador_catalog.logistics.bronze_tabula_actualiza_procesa_pesca_acua_13agosto2018_wout_carr table saved\n\nTruncating the read of 'tabula-ACTUALIZA_PROCESA_PESCA_ACUA_24OCTUBRE2017.csv' to just review its header\n[Truncated to first 1000 bytes]\necuador_catalog.logistics.bronze_tabula_actualiza_procesa_pesca_acua_24octubre2017 table saved\n\nTruncating the read of 'ec_processing_facilities.csv' to just review its header\n[Truncated to first 1000 bytes]\necuador_catalog.logistics.bronze_ec_processing_facilities table saved\n\nTruncating the read of 'processing_facilities_cleaned.csv' to just review its header\n[Truncated to first 1000 bytes]\necuador_catalog.logistics.bronze_processing_facilities_cleaned table saved\n\nTruncating the read of 'CHINA_matches.csv' to just review its header\n[Truncated to first 1000 bytes]\necuador_catalog.logistics.bronze_china_matches table saved\n\nTruncating the read of 'CHINA_non_matches_cleaned.csv' to just review its header\n[Truncated to first 1000 bytes]\necuador_catalog.logistics.bronze_china_non_matches_cleaned table saved\n\nTruncating the read of 'tabula-FFP_EC_es(Establecimientos_autorizados_EC).csv' to just review its header\n[Truncated to first 1000 bytes]\necuador_catalog.logistics.bronze_tabula_ffp_ec_es_establecimientos_autorizados_ec table saved\n\nTruncating the read of 'ec_processing_facilities_eu.csv' to just review its header\n[Truncated to first 1000 bytes]\necuador_catalog.logistics.bronze_ec_processing_facilities_eu table saved\n"
     ]
    }
   ],
   "source": [
    "for csv_record in files_list:\n",
    "    delimiter, temp_csv_path = pre_process_csv(csv_record)\n",
    "    create_table_from_csv(source_filepath=csv_record, catalog=catalog, db=db, tmp_csv_filepath=temp_csv_path, delimiter=delimiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b7a06c9-be45-47ae-9b37-f6ff4dde17a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Ecuador Trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2122320-a170-4249-a794-c4c703b4274b",
     "showTitle": false,
     "title": "Trade"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n***\nWarning: s3a://uu-trase-data/ecuador/trade/cd/export/2013/CD_ECUADOR_2013.csv is bigger than 50M, header replacement has to be done manually for the moment\n***\n\n***\nWarning: s3a://uu-trase-data/ecuador/trade/cd/export/2014/CD_ECUADOR_2014.csv is bigger than 50M, header replacement has to be done manually for the moment\n***\n\n***\nWarning: s3a://uu-trase-data/ecuador/trade/cd/export/2015/CD_ECUADOR_2015.csv is bigger than 50M, header replacement has to be done manually for the moment\n***\n\n***\nWarning: s3a://uu-trase-data/ecuador/trade/cd/export/2016/CD_ECUADOR_2016.csv is bigger than 50M, header replacement has to be done manually for the moment\n***\n\n***\nWarning: s3a://uu-trase-data/ecuador/trade/cd/export/2017/CD_ECUADOR_2017.csv is bigger than 50M, header replacement has to be done manually for the moment\n***\n\n***\nWarning: s3a://uu-trase-data/ecuador/trade/cd/export/2018/CD_ECUADOR_2018.csv is bigger than 50M, header replacement has to be done manually for the moment\n***\n\n***\nWarning: s3a://uu-trase-data/ecuador/trade/cd/export/2019/CD_ECUADOR_2019.csv is bigger than 50M, header replacement has to be done manually for the moment\n***\n\n***\nWarning: s3a://uu-trase-data/ecuador/trade/cd/export/cleaned/CD_ECUADOR_2013.csv is bigger than 50M, header replacement has to be done manually for the moment\n***\n\n***\nWarning: s3a://uu-trase-data/ecuador/trade/cd/export/cleaned/CD_ECUADOR_2014.csv is bigger than 50M, header replacement has to be done manually for the moment\n***\n\n***\nWarning: s3a://uu-trase-data/ecuador/trade/cd/export/cleaned/CD_ECUADOR_2015.csv is bigger than 50M, header replacement has to be done manually for the moment\n***\n\n***\nWarning: s3a://uu-trase-data/ecuador/trade/cd/export/cleaned/CD_ECUADOR_2016.csv is bigger than 50M, header replacement has to be done manually for the moment\n***\n\n***\nWarning: s3a://uu-trase-data/ecuador/trade/cd/export/cleaned/CD_ECUADOR_2017.csv is bigger than 50M, header replacement has to be done manually for the moment\n***\n\n***\nWarning: s3a://uu-trase-data/ecuador/trade/cd/export/cleaned/CD_ECUADOR_2018.csv is bigger than 50M, header replacement has to be done manually for the moment\n***\n\n***\nWarning: s3a://uu-trase-data/ecuador/trade/cd/export/cleaned/CD_ECUADOR_2019.csv is bigger than 50M, header replacement has to be done manually for the moment\n***\n[]\n"
     ]
    }
   ],
   "source": [
    "catalog = \"ecuador_catalog\"\n",
    "db = \"trade\"\n",
    "dir_path = f\"{data_bucket}/ecuador/trade\"\n",
    "files_list = []\n",
    "list_files_recursively(dir_path, files_list)\n",
    "print(files_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c14b1e0-4ace-4185-bd7e-8f6c48c3ede2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Ecuador Spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67e2f299-98c9-4136-a649-0b8ac5974369",
     "showTitle": false,
     "title": "Trade"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Info: s3a://uu-trase-data/ecuador/spatial/BOUNDARIES/originals/nxcantones.dbf is not a csv file. Skipping it***\n***Info: s3a://uu-trase-data/ecuador/spatial/BOUNDARIES/originals/nxparroquias.dbf is not a csv file. Skipping it***\n***Info: s3a://uu-trase-data/ecuador/spatial/BOUNDARIES/originals/nxprovincias.dbf is not a csv file. Skipping it***\n['s3a://uu-trase-data/ecuador/spatial/BOUNDARIES/city_to_parish.csv', 's3a://uu-trase-data/ecuador/spatial/BOUNDARIES/out/ec_parishes.csv']\n"
     ]
    }
   ],
   "source": [
    "catalog = \"ecuador_catalog\"\n",
    "db = \"spatial\"\n",
    "dir_path = f\"{data_bucket}/ecuador/spatial\"\n",
    "files_list = []\n",
    "list_files_recursively(dir_path, files_list)\n",
    "print(files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdf93c5d-7b0c-40aa-b54b-2ed751a7a00b",
     "showTitle": true,
     "title": "Actual creation"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTruncating the read of 'city_to_parish.csv' to just review its header\necuador_catalog.spatial.bronze_city_to_parish table saved\n\nTruncating the read of 'ec_parishes.csv' to just review its header\n[Truncated to first 2000 bytes]\necuador_catalog.spatial.bronze_ec_parishes table saved\n"
     ]
    }
   ],
   "source": [
    "for csv_record in files_list:\n",
    "    delimiter, temp_csv_path = pre_process_csv(csv_record)\n",
    "    create_table_from_csv(source_filepath=csv_record, catalog=catalog, db=db, tmp_csv_filepath=temp_csv_path, delimiter=delimiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0f2d53d-d48e-4b82-8cea-b28d401c1a6e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Ecuador Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16f19285-8c8a-4817-a176-8c2b86876d23",
     "showTitle": false,
     "title": "Trade"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s3a://uu-trase-data/ecuador/production/crop_maps/production/shrimp_pond_maps/out/ec_production_per_parish.csv', 's3a://uu-trase-data/ecuador/production/crop_maps/production/shrimp_pond_maps/out/ec_shrimp_area_per_parish.csv', 's3a://uu-trase-data/ecuador/production/crop_maps/production/shrimp_pond_maps/out/ec_shrimp_ponds.csv', 's3a://uu-trase-data/ecuador/production/crop_maps/production/shrimp_pond_maps/out/shrimp_ponds_cleaned.csv']\n"
     ]
    }
   ],
   "source": [
    "catalog = \"ecuador_catalog\"\n",
    "db = \"production\"\n",
    "dir_path = f\"{data_bucket}/ecuador/production\"\n",
    "files_list = []\n",
    "list_files_recursively(dir_path, files_list)\n",
    "print(files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "368f37ba-9444-4586-a99d-70eecd63bb55",
     "showTitle": true,
     "title": "Actual creation"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTruncating the read of 'ec_production_per_parish.csv' to just review its header\n[Truncated to first 2000 bytes]\necuador_catalog.production.bronze_ec_production_per_parish table saved\n\nTruncating the read of 'ec_shrimp_area_per_parish.csv' to just review its header\necuador_catalog.production.bronze_ec_shrimp_area_per_parish table saved\n\nTruncating the read of 'ec_shrimp_ponds.csv' to just review its header\n[Truncated to first 2000 bytes]\necuador_catalog.production.bronze_ec_shrimp_ponds table saved\n\nTruncating the read of 'shrimp_ponds_cleaned.csv' to just review its header\n[Truncated to first 2000 bytes]\necuador_catalog.production.bronze_shrimp_ponds_cleaned table saved\n"
     ]
    }
   ],
   "source": [
    "for csv_record in files_list:\n",
    "    delimiter, temp_csv_path = pre_process_csv(csv_record)\n",
    "    create_table_from_csv(source_filepath=csv_record, catalog=catalog, db=db, tmp_csv_filepath=temp_csv_path, delimiter=delimiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06450b86-9183-4f98-8560-d1b7771f1dc6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Ecuador Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "837f65b8-c5a9-47ab-b1d1-0b82b6f02dac",
     "showTitle": false,
     "title": "Trade"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Info: s3a://uu-trase-data/ecuador/metadata/ciiu/in/Sector Sums_with classification.xlsx is not a csv file. Skipping it***\n[]\n"
     ]
    }
   ],
   "source": [
    "catalog = \"ecuador_catalog\"\n",
    "db = \"metadata\"\n",
    "dir_path = f\"{data_bucket}/ecuador/metadata\"\n",
    "files_list = []\n",
    "list_files_recursively(dir_path, files_list)\n",
    "print(files_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66db7b8b-f5c0-4fa2-9212-463bf92d47d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Ecuador Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b0fd544-d0e8-4d2b-8cfe-a94fe6abf5ed",
     "showTitle": false,
     "title": "Trade"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s3a://uu-trase-data/ecuador/indicators/out/Clark_Lab_Change_Persistence_Datasets.csv', 's3a://uu-trase-data/ecuador/indicators/out/Clark_Lab_Datasets_v3.csv']\n"
     ]
    }
   ],
   "source": [
    "catalog = \"ecuador_catalog\"\n",
    "db = \"indicators\"\n",
    "dir_path = f\"{data_bucket}/ecuador/indicators\"\n",
    "files_list = []\n",
    "list_files_recursively(dir_path, files_list)\n",
    "print(files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53b314a3-3914-42d3-a7b3-7fb1a2bc5655",
     "showTitle": true,
     "title": "Actual creation"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTruncating the read of 'Clark_Lab_Change_Persistence_Datasets.csv' to just review its header\n[Truncated to first 2000 bytes]\necuador_catalog.indicators.bronze_clark_lab_change_persistence_datasets table saved\n\nTruncating the read of 'Clark_Lab_Datasets_v3.csv' to just review its header\n[Truncated to first 2000 bytes]\nWrote 153201 bytes.\necuador_catalog.indicators.bronze_clark_lab_datasets_v3 table saved\n"
     ]
    }
   ],
   "source": [
    "for csv_record in files_list:\n",
    "    delimiter, temp_csv_path = pre_process_csv(csv_record)\n",
    "    create_table_from_csv(source_filepath=csv_record, catalog=catalog, db=db, tmp_csv_filepath=temp_csv_path, delimiter=delimiter)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "load_csvs_in_unity_catalog",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
