{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df16a3f8-2282-4635-bde4-2dc60fbab88b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Overview\n",
    "This notebook will read from a Postgres database, and keep an updated replication of a selection of tables and views into the Unity Catalog.\n",
    "\n",
    "This notebook can be run manually or through a scheduled job.\n",
    "\n",
    "It could also be converted to a python script, and/or loaded in github, and have the scheduled jub run it from there.\n",
    "\n",
    "## Notes\n",
    "* If the table doesn't exist at the destination in Databricks, it will create it\n",
    "* If a refresh time field is specified, it will use it to check if a new version is available from the current replicated one.\n",
    "* Currently, the script assumes that the refresh time value is the same in the whole source table (as it currently is). In case this is changed, this script would have to change to reflect that\n",
    "* If there needs to be a refresh, it will load the source table(s) completely, create a hash value for uniquely identifying records, and comparing this with the records in the destination\n",
    "* The tables at the destination have an additional `dbr_hash` field to identify which records need to be inserted or deleted from the destination table based on changes from the source table\n",
    "* The destination tables will have a version history recording the inserts and deletes (based on insert, deletes or updates of the source table)\n",
    "* The `last_refresh_time` field is dropped from the destination table, and saved as a table property called `source_timestamp`. As currently this value is the same for all fields in the source table, and it would take storage innecesarily upon every version update (specially on big tables), it can be practical to drop it.\n",
    "\n",
    "## Notes (2)\n",
    "* To reference a table with special characters in its name (e.g. \"-\") in Postgres, use double quotes (for example: `*****.\"*****-dashboard\"`)\n",
    "* To reference it within Spark / Databricks Delta Table, use back-tick (for example: *****.`` `*****-dashboard` ``)\n",
    "## To-do\n",
    "* Allow for passing parameters through a Databricks task: for example for specifying the tables to process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c31f3a1-043e-4a99-9ab8-d0ff6988bb17",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Supporting definitions and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "085044dd-842e-4735-a84e-f01e2924b88f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat_ws, hash, when\n",
    "from datetime import datetime\n",
    "\n",
    "# Set partitions (parallel postgres processes) to the number of available cores in the Cluster\n",
    "# If going beyond ~50, there needs to be special considerations\n",
    "num_partitions = 8\n",
    "\n",
    "pg_host = dbutils.secrets.get(scope=\"****\", key=\"****\")\n",
    "pg_port = \"****\"\n",
    "pg_database = \"****\"\n",
    "pg_user = dbutils.secrets.get(scope=\"****\", key=\"****\")\n",
    "pg_pass = dbutils.secrets.get(scope=\"****\", key=\"****\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2196c3cf-e35e-429e-9b5a-9123344a59b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_table(table, schema, query):\n",
    "    \"\"\"Return a DataFrame with the table name. Takes connection details from global variables\"\"\"\n",
    "    dbtable = f\"({query}) AS result_table\"\n",
    "\n",
    "    table_df = (spark.read\n",
    "        .format(\"postgresql\")\n",
    "        .option(\"host\", pg_host)\n",
    "        .option(\"port\", pg_port)\n",
    "        .option(\"database\", pg_database)\n",
    "        .option(\"user\",  pg_user)\n",
    "        .option(\"password\", pg_pass)\n",
    "        .option(\"numPartitions\", num_partitions)\n",
    "        .option(\"dbtable\", dbtable)\n",
    "        .load()\n",
    "    )\n",
    "    return table_df\n",
    "  \n",
    "def get_big_table(table, schema, query, partitions_config):\n",
    "    \"\"\"Return a DataFrame with the table name. Takes connection details from global variables.\n",
    "    Requires partition information to process parts of the tables in parallel\"\"\"\n",
    "    \n",
    "    num_partitions = partitions_config['num_partitions']\n",
    "    column_partition = partitions_config['column_partition']\n",
    "    lower_bound = partitions_config['lower_bound']\n",
    "    upper_bound = partitions_config['upper_bound']\n",
    "    \n",
    "    dbtable = f\"({query}) AS result_table\"\n",
    "\n",
    "    table_df = (spark.read\n",
    "        .format(\"postgresql\")\n",
    "        .option(\"host\", pg_host)\n",
    "        .option(\"port\", pg_port)\n",
    "        .option(\"database\", pg_database)\n",
    "        .option(\"user\",  pg_user)\n",
    "        .option(\"password\", pg_pass)\n",
    "        .option(\"dbtable\", dbtable)\n",
    "        .option(\"fetchsize\", \"10000\")\n",
    "        .option(\"partitionColumn\", column_partition)\n",
    "        .option(\"numPartitions\", num_partitions)\n",
    "        .option(\"lowerBound\", lower_bound)\n",
    "        .option(\"upperBound\", upper_bound)\n",
    "        .option(\"stringtype\", \"unspecified\")\n",
    "        .load()\n",
    "    )\n",
    "    return table_df\n",
    "\n",
    "def dest_table_exists(table, schema, catalog):\n",
    "    schema_tables = spark.sql(f\"SHOW TABLES IN {catalog}.{schema}\")\n",
    "    return bool(schema_tables.filter(schema_tables.tableName == f\"{table}\").collect())\n",
    "\n",
    "def get_column_bounds(table, schema, column_partition):\n",
    "    q_bounds = (f'SELECT MIN(\"{column_partition}\") AS lower_bound, '\n",
    "                f'MAX(\"{column_partition}\") AS upper_bound '\n",
    "                f'FROM {schema}.\"{table}\"')\n",
    "    df_bounds = get_table(table, schema, q_bounds)\n",
    "    lower_bound = df_bounds.head()['lower_bound']\n",
    "    upper_bound = df_bounds.head()['upper_bound']\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "def get_schema_info(dataframe, exclude_columns):\n",
    "    '''Returns the schema of the dataframe, using python .dtypes information\n",
    "    (field name and type), excluding the column names in 'exclude_columns'  '''\n",
    "    schema_info = []\n",
    "    for column in dataframe.dtypes:\n",
    "        if column[0] not in exclude_columns:\n",
    "            schema_info.append(column)\n",
    "    return schema_info \n",
    "\n",
    "def register_table_properties(table, schema, catalog):\n",
    "    table_description = (f\"Table `{table}` replicated from Postgres. \"\n",
    "                    f\"Version history can be seen using `DESCRIBE HISTORY {catalog}.{schema}.{table}`. \"\n",
    "                    \"Use the @ syntax to specify the timestamp or version as part of the table name to query \"\n",
    "                    \"a specific version. Specify a version after `@` by prepending a `v` to the version number. \"\n",
    "                    \"Timestamps must be in `yyyyMMddHHmmssSSS` format. \"\n",
    "                    f\"Examples: `SELECT * FROM {catalog}.{schema}.{table}@20190101000000000`.... \"\n",
    "                    f\"`SELECT * FROM {catalog}.{schema}.{table}@v123` \")\n",
    "    q_register_description = (f\"COMMENT ON TABLE {catalog}.{schema}.`{table}` IS \\\"{table_description}\\\"\")\n",
    "    spark.sql(q_register_description)\n",
    "    # Set retention policy to keep history\n",
    "    q_retention_policy = (f\"ALTER TABLE {catalog}.{schema}.`{table}` \"\n",
    "                        f\"SET TBLPROPERTIES ('delta.logRetentionDuration'='interval 50000 weeks')\")\n",
    "    spark.sql(q_retention_policy)\n",
    "\n",
    "def register_source_timestamp_property(table, schema, catalog, source_time):\n",
    "        if (source_time != \"\"):\n",
    "            # Register the source refresh field timestamp as a Table property\n",
    "            q_register_source_timestamp = (f\"ALTER TABLE {catalog}.{schema}.`{table}` \"\n",
    "                                    f\"SET TBLPROPERTIES ('source_timestamp'='{source_time}')\")\n",
    "            spark.sql(q_register_source_timestamp)\n",
    "            print(f\"Registered the table property of '{table}': 'source_timestamp'= {source_time}\")\n",
    "\n",
    "def process_table (table, schema, catalog, refresh_field=\"\", partitions_config=\"\"):\n",
    "    '''Loads table from Postgres into a Spark Dataframe, and merges it into the corresponding\n",
    "    Databricks destination table. Assumes the table name and the schema have the same name\n",
    "    in Postgres as in Databricks selected catalog'''\n",
    "    refresh = False # Variable to indicate if there needs to be a full refresh\n",
    "    source_time = \"\"\n",
    "    print(f\"\\n*** Processing table {schema}.{table} ***\")\n",
    "\n",
    "    if (refresh_field != \"\"):\n",
    "        # Check the source time\n",
    "        q_source_time = f'SELECT {refresh_field} FROM {schema}.\"{table}\" WHERE {refresh_field} IS NOT NULL LIMIT 1'\n",
    "        df_source_time = get_table(table, schema, q_source_time)\n",
    "        source_time = df_source_time.head()[f'{refresh_field}']\n",
    "    \n",
    "    # Check if the destination table exists. If not, will create it later on with the full refresh\n",
    "    destination_table_exists = dest_table_exists(table, schema, catalog)\n",
    "    if (destination_table_exists == False):\n",
    "        refresh = True\n",
    "        print(f\"The destination table '{schema}.{table}' doesn't exist. Creating it and doing a full refresh\")\n",
    "    \n",
    "    if (refresh == False and refresh_field != \"\"):\n",
    "        df_dest_time = spark.sql(f\"SHOW TBLPROPERTIES {catalog}.{schema}.`{table}` ('source_timestamp')\")\n",
    "        # Check if there is need to refresh the destination table based on 'refresh_field'\n",
    "        str_destination_time = df_dest_time.head()['value']\n",
    "        destination_time = datetime.strptime(str_destination_time, '%Y-%m-%d %H:%M:%S.%f')\n",
    "        \n",
    "        if (source_time > destination_time):\n",
    "            print(f\"The destination table '{schema}.{table}' is outdated, will review source table for updates\")\n",
    "        elif (source_time == destination_time):\n",
    "            print(f\"The destination table '{schema}.{table}' seems up to date, with '{refresh_field}': {destination_time}. No need to update it\")\n",
    "            return\n",
    "        else:\n",
    "            print(f\"Inconsistency comparing '{refresh_field}' of '{schema}.{table}'. Please check. Source Table value: {source_time} . Destination table value: {destination_time}\")\n",
    "\n",
    "    # Get the source table schema\n",
    "    q_load_schema = f'SELECT * FROM {schema}.\"{table}\" WHERE 1=2 '\n",
    "    df_source_schema = get_table(table, schema, q_load_schema)\n",
    "    exclude_columns = [f'{refresh_field}', 'dbr_hash']\n",
    "    source_schema = get_schema_info(df_source_schema, exclude_columns)\n",
    "    # Take the field names of the schema for doing the source full SELECT\n",
    "    source_col_names = [column[0] for column in source_schema]\n",
    "    str_source_cols = \", \".join(source_col_names)\n",
    "    \n",
    "    # Load all the source table (excluding refresh field if existing)\n",
    "    # Bigger tables are read in parallel through partitions, specified in 'partitions_config'\n",
    "    q_load_source = f'SELECT {str_source_cols} FROM {schema}.\"{table}\"'\n",
    "    if (partitions_config == \"\"):\n",
    "        df_source = get_table(table, schema, q_load_source)\n",
    "    else:  \n",
    "        print(f\"Processing table '{table}', reading it through custom partitions for faster load and processing\")\n",
    "        df_source = get_big_table(table, schema, q_load_source, partitions_config)\n",
    "\n",
    "    print(\"Creating a hash column to uniquely identify records. Might take some time depending on the table size\")\n",
    "    df_source = df_source.withColumn(\"dbr_hash\", hash(concat_ws(\",\", *df_source.columns)))\n",
    "    df_source.createOrReplaceTempView(\"temp_source_table\") # Register it in sparksql\n",
    "\n",
    "    # Compare the source and destination schema. If different, do a full refresh\n",
    "    if (destination_table_exists == True):\n",
    "        df_destination = spark.read.table(f\"{catalog}.{schema}.`{table}`\")\n",
    "        destination_schema = get_schema_info(df_destination, exclude_columns=['dbr_hash'])\n",
    "        if (source_schema != destination_schema):\n",
    "            refresh = True\n",
    "            print(\"The schema of the source table seems to have changed. Doing a full refresh with the new structure\")\n",
    "            print(f\"Source schema: {source_schema}\\n Destination schema: {destination_schema}\")\n",
    "\n",
    "    # Do a full refresh of the destination table if specified\n",
    "    if (refresh == True):\n",
    "        df_source.write.mode(\"overwrite\") \\\n",
    "                            .option(\"overwriteSchema\", \"True\") \\\n",
    "                            .saveAsTable(f\"{catalog}.{schema}.`{table}`\")\n",
    "        print(f\"Made a full refresh of '{schema}.{table}' in Databricks, and added a 'dbr_hash' column for uniquely identifying records\")\n",
    "        register_table_properties(table, schema, catalog)\n",
    "        register_source_timestamp_property(table, schema, catalog, source_time)\n",
    "        return\n",
    "\n",
    "    # If not doing a full refresh, continue with comparisons\n",
    "    \n",
    "    # Load hashes from destination table to do comparisons\n",
    "    df_destination_hashes = spark.sql(f\"SELECT dbr_hash FROM {catalog}.{schema}.`{table}`\")\n",
    "    df_destination_hashes.createOrReplaceTempView(\"temp_destination_hashes\") # Register it in sparksql\n",
    "\n",
    "    # Identify the new or updated records from the source table and insert them in the actual destination table\n",
    "    q_updated_hashes = f\"SELECT dbr_hash FROM temp_source_table EXCEPT SELECT dbr_hash FROM temp_destination_hashes\"\n",
    "    df_updated_hashes = spark.sql(q_updated_hashes)\n",
    "    df_updated_hashes.createOrReplaceTempView(\"updated_hashes\")\n",
    "    q_updated_records = (f\"SELECT * FROM temp_source_table WHERE dbr_hash IN \"\n",
    "                    \"(SELECT dbr_hash FROM updated_hashes)\")\n",
    "    df_updated_records = spark.sql(q_updated_records)\n",
    "\n",
    "    if (df_updated_records.count() > 0):\n",
    "        df_updated_records.createOrReplaceTempView(\"updated_records\")\n",
    "        # Make the changes to the destination table\n",
    "        insert_result = spark.sql(f\"INSERT INTO {catalog}.{schema}.`{table}` SELECT * FROM updated_records\")\n",
    "        print(f\"Records have been inserted or updated at '{table}' destination table\")\n",
    "    else: \n",
    "        print(f\"No records needed to be inserted or updated at '{table}' destination table\")\n",
    "\n",
    "    # If the source and destination tables are now the same size, there is no need to delete records\n",
    "    df_destination_size = spark.sql(f\"SELECT COUNT(*) AS records FROM {catalog}.{schema}.`{table}`\")\n",
    "    destination_size = df_destination_size.head()['records']\n",
    "    if (df_source.count() == destination_size):\n",
    "        print(f\"No records have needed to be deleted from the destination '{table}' table\")\n",
    "        register_source_timestamp_property(table, schema, catalog, source_time)\n",
    "        return\n",
    "\n",
    "    # Otherwise, identify and delete records from the destination table that are not in the source table\n",
    "    q_hashes_todel = (f\"SELECT dbr_hash FROM {catalog}.{schema}.`{table}` \"\n",
    "                        \"EXCEPT SELECT dbr_hash FROM temp_source_table\")\n",
    "    df_hashes_todel = spark.sql(q_hashes_todel)\n",
    "    df_hashes_todel.createOrReplaceTempView(f\"hashes_to_del\")\n",
    "    q_delete = (f\"DELETE FROM {catalog}.{schema}.`{table}` WHERE dbr_hash IN \"\n",
    "                \"(SELECT dbr_hash FROM hashes_to_del)\")\n",
    "    delete_result = spark.sql(q_delete)\n",
    "    print(f\"Records have been deleted from the destination '{table}' table as they were outdated\")\n",
    "\n",
    "    register_source_timestamp_property(table, schema, catalog, source_time)\n",
    "    return\n",
    "\n",
    "def process_table_list(table_list, schema, databricks_catalog, refresh_field=\"\"):\n",
    "    for table in table_list:\n",
    "        process_table(table, schema, databricks_catalog, refresh_field=refresh_field)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbd01fe5-f588-4726-b569-9a8084889084",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Actual update of tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "132bc957-ff22-4908-9552-00e127491ad8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pg_host = dbutils.secrets.get(scope=\"****\", key=\"****\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ee8b017-529c-4de6-ac17-97641c3c774e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Small tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63c9ecc6-9c53-49a4-b871-ddb174201a83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n*** Processing table *****.commitments ***\nThe destination table '*****.commitments' seems up to date, with 'last_refresh_time': 2023-04-09 04:03:29.342338. No need to update it\n\n*** Processing table *****.commodities ***\nThe destination table '*****.commodities' seems up to date, with 'last_refresh_time': 2023-04-09 04:03:29.335084. No need to update it\n\n*** Processing table *****.*****-dashboard ***\nThe destination table '*****.*****-dashboard' seems up to date, with 'last_refresh_time': 2023-04-09 04:03:29.337951. No need to update it\n\n*** Processing table *****.logistics ***\nThe destination table '*****.logistics' seems up to date, with 'last_refresh_time': 2023-04-09 04:03:29.331664. No need to update it\n"
     ]
    }
   ],
   "source": [
    "databricks_catalog = \"****\"\n",
    "\n",
    "schema = \"*****\"\n",
    "*****tables = [\"commitments\", \"commodities\", \"*****-dashboard\", \"logistics\"]\n",
    "process_table_list(*****tables, schema, databricks_catalog, refresh_field=\"last_refresh_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cc7e6f8-4ab6-4652-a171-7e74f67fa3cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Small views\n",
    "Note they don't have a refresh field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dea151bb-c19f-4bfd-af88-98fffb6dcf62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n*** Processing table views.datasets ***\nCreating a hash column to uniquely identify records. Might take some time depending on the table size\nNo records needed to be inserted or updated at 'datasets' destination table\nNo records have needed to be deleted from the destination 'datasets' table\n\n*** Processing table views.f500 ***\nCreating a hash column to uniquely identify records. Might take some time depending on the table size\nNo records needed to be inserted or updated at 'f500' destination table\nNo records have needed to be deleted from the destination 'f500' table\n\n*** Processing table views.indicators_type ***\nCreating a hash column to uniquely identify records. Might take some time depending on the table size\nNo records needed to be inserted or updated at 'indicators_type' destination table\nNo records have needed to be deleted from the destination 'indicators_type' table\n\n*** Processing table views.rejected_name_matches ***\nCreating a hash column to uniquely identify records. Might take some time depending on the table size\nNo records needed to be inserted or updated at 'rejected_name_matches' destination table\nNo records have needed to be deleted from the destination 'rejected_name_matches' table\n\n*** Processing table views.supply_chains_pipeline ***\nCreating a hash column to uniquely identify records. Might take some time depending on the table size\nNo records needed to be inserted or updated at 'supply_chains_pipeline' destination table\nNo records have needed to be deleted from the destination 'supply_chains_pipeline' table\n"
     ]
    }
   ],
   "source": [
    "schema = \"views\"\n",
    "view_tables = [\"datasets\", \"f500\", \"indicators_type\", \"rejected_name_matches\", \"supply_chains_pipeline\"]\n",
    "process_table_list(view_tables, schema, databricks_catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25c78c27-5df2-4ddc-ac3b-11bd556b5a84",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Big tables (>100MB)\n",
    "For the bigger tables, specifying partitioning information including `column_partition`, with its corresponding `lower_bound` and `upper_bound` so they can be read and processed in `num_partitions` chunks processed in parallel.\n",
    "\n",
    "It is recommended that the `column_partition` is an integer or date, and that its evenly distributed so that each partition processes roughly the same amount of records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0da9d92-14d5-44c0-8282-23cd4aa8edb6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### supply-chains-latest\n",
    "Notes:\n",
    "* Full refresh takes 1.4 minutes in single node, 1 minute in a two node cluster\n",
    "* Using `last_refresh_time` for partitioning\n",
    "* Source table size: 3.11 GB\n",
    "* Destination table size: 250MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9129d750-172c-4760-8142-c54ab4dc9654",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n*** Processing table *****.supply-chains-latest ***\nThe destination table '*****.supply-chains-latest' seems up to date, with 'last_refresh_time': 2023-04-06 09:14:22.801232. No need to update it\n"
     ]
    }
   ],
   "source": [
    "catalog = \"*****\"\n",
    "schema = \"*****\"\n",
    "table = \"supply-chains-latest\"\n",
    "\n",
    "# Will use the row number for dividing partitions\n",
    "num_partitions = 18 # rule of thumb: 2 - 3 times number of worker cpus\n",
    "column_partition = \"row_number\"\n",
    "lower_bound, upper_bound = get_column_bounds(table, schema, column_partition)\n",
    "\n",
    "partitions_config = {\n",
    "    \"num_partitions\": num_partitions, \n",
    "    \"column_partition\": column_partition,\n",
    "    \"lower_bound\": lower_bound,\n",
    "    \"upper_bound\": upper_bound\n",
    "}\n",
    "\n",
    "process_table(table, schema, catalog, refresh_field=\"last_refresh_time\", partitions_config=partitions_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dea21e0-67f1-4133-9c3a-7fef236ec397",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### supply-chains-concatenated\n",
    "Notes:\n",
    "* Currently using `commodity_id` as partitioning column to split the table for read, though as its not evenly distributed, it puts all the burden of processing in two of the table partitions\n",
    "  * For improving this, find another more evenly distributed column. Or split in an array of reads based on a hashed value [(example link - using scala)](https://dzlab.github.io/spark/2022/02/10/spark-jdbc-partitioning/)\n",
    "* Takes 2.86 minutes for a full refresh in a single node, 2 minutes in a 2 node cluster\n",
    "* Destination table size: 380MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7843df12-327f-4e37-b11f-bdb0808e5ce2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n*** Processing table *****.supply-chains-concatenated ***\nThe destination table '*****.supply-chains-concatenated' seems up to date, with 'last_refresh_time': 2023-04-06 09:14:22.801232. No need to update it\n"
     ]
    }
   ],
   "source": [
    "catalog = \"****\"\n",
    "schema = \"*****\"\n",
    "table = \"supply-chains-concatenated\"\n",
    "\n",
    "num_partitions = 18 # rule of thumb: 2 - 3 times number of worker cpus (or number of distinct partitions)\n",
    "column_partition = \"commodity_id\"\n",
    "lower_bound, upper_bound = get_column_bounds(table, schema, column_partition)\n",
    "\n",
    "partitions_config = {\n",
    "    \"num_partitions\": num_partitions, \n",
    "    \"column_partition\": column_partition,\n",
    "    \"lower_bound\": lower_bound,\n",
    "    \"upper_bound\": upper_bound\n",
    "}\n",
    "\n",
    "process_table(table, schema, catalog, refresh_field=\"last_refresh_time\", partitions_config=partitions_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2dcdb6a-50cd-4638-bcc6-747a5464a0d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### regional-indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b73f7857-4749-41c5-89be-540330d60d53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n*** Processing table *****.regional-indicators ***\nThe destination table '*****.regional-indicators' doesn't exist. Creating it and doing a full refresh\nProcessing table 'regional-indicators', reading it through custom partitions for faster load and processing\nCreating a hash column to uniquely identify records. Might take some time depending on the table size\nMade a full refresh of '*****.regional-indicators' in Databricks, and added a 'dbr_hash' column for uniquely identifying records\nRegistered the table property of 'regional-indicators': 'source_timestamp'= 2023-04-09 04:03:29.320122\n"
     ]
    }
   ],
   "source": [
    "catalog = \"****\"\n",
    "schema = \"*****\"\n",
    "table = \"regional-indicators\"\n",
    "\n",
    "num_partitions = 18 # rule of thumb: 2 - 3 times number of worker cpus (or number of distinct partitions)\n",
    "column_partition = \"level\"\n",
    "lower_bound, upper_bound = get_column_bounds(table, schema, column_partition)\n",
    "\n",
    "partitions_config = {\n",
    "    \"num_partitions\": num_partitions, \n",
    "    \"column_partition\": column_partition,\n",
    "    \"lower_bound\": lower_bound,\n",
    "    \"upper_bound\": upper_bound\n",
    "}\n",
    "\n",
    "process_table(table, schema, catalog, refresh_field=\"last_refresh_time\", partitions_config=partitions_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "556f4c68-7d06-4809-b582-d1b958c8a74f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "539c59ec-a62e-4508-a441-985a118e4f9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n*** Processing table *****.regions ***\nThe destination table '*****.regions' doesn't exist. Creating it and doing a full refresh\nProcessing table 'regions', reading it through custom partitions for faster load and processing\nCreating a hash column to uniquely identify records. Might take some time depending on the table size\nMade a full refresh of '*****.regions' in Databricks, and added a 'dbr_hash' column for uniquely identifying records\nRegistered the table property of 'regions': 'source_timestamp'= 2023-04-09 04:04:55.366554\n"
     ]
    }
   ],
   "source": [
    "catalog = \"*****\"\n",
    "schema = \"*****\"\n",
    "table = \"regions\"\n",
    "\n",
    "num_partitions = 18 # note however that the 'level' column has fewer values\n",
    "column_partition = \"level\"\n",
    "lower_bound, upper_bound = get_column_bounds(table, schema, column_partition)\n",
    "\n",
    "partitions_config = {\n",
    "    \"num_partitions\": num_partitions, \n",
    "    \"column_partition\": column_partition,\n",
    "    \"lower_bound\": lower_bound,\n",
    "    \"upper_bound\": upper_bound\n",
    "}\n",
    "\n",
    "process_table(table, schema, catalog, refresh_field=\"last_refresh_time\", partitions_config=partitions_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecd537be-66bb-4c91-914a-f3cc665d5590",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### traders\n",
    "Note that as it doesn't have an integer column to use for partitioning it while reading, it will be read all in a single process.\n",
    "\n",
    "Another option is to do multiple reads based on a hash value of one of the string columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "415cae01-76ac-4cd9-8f07-0c9f913f8b70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n*** Processing table *****.traders ***\nThe destination table '*****.traders' doesn't exist. Creating it and doing a full refresh\nCreating a hash column to uniquely identify records. Might take some time depending on the table size\nMade a full refresh of '*****.traders' in Databricks, and added a 'dbr_hash' column for uniquely identifying records\nRegistered the table property of 'traders': 'source_timestamp'= 2023-04-06 09:14:21.236168\n"
     ]
    }
   ],
   "source": [
    "catalog = \"****\"\n",
    "schema = \"*****\"\n",
    "table = \"traders\"\n",
    "\n",
    "process_table(table, schema, catalog, refresh_field=\"last_refresh_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc4ab3bf-e7bf-411a-9f36-cc616969d2e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Big views\n",
    "Review if including `country_node_attributes_view` and `flows_long`, as I couldn't do a full count on them to see the size (stopped it after 10 minutes running)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2cd8c10-1cd3-4332-8f63-5bc81bcd9013",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# From here on, test and debugging cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3bd1c30-214b-4fa2-b45c-a3557b5ee014",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Commitments updates tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba59deb9-a169-4c7e-9cc1-31deefa9edb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# host is actual postgres server\n",
    "pg_host = dbutils.secrets.get(scope=\"****\", key=\"****)\n",
    "catalog = \"*****\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d28020f3-df9a-47e6-af77-3539cca26da3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n*** Processing table *****.commitments ***\nThe destination table '*****.commitments' is outdated, will review source table for updates\nCreating a hash column to uniquely identify records. Might take some time depending on the table size\nRecords have been inserted or updated at 'commitments' destination table\nRecords have been deleted from the destination 'commitments' table as they were outdated\nRegistered the table property of 'commitments': 'source_timestamp'= 2023-04-13 09:00:38.682497\n"
     ]
    }
   ],
   "source": [
    "schema = \"*****\"\n",
    "table = \"commitments\"\n",
    "\n",
    "process_table(table, schema, catalog, refresh_field=\"last_refresh_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "daeb322c-0e57-4da7-8bbc-58717676266a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## supply-chains-latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "772f949e-96ab-41bc-b6af-6e81fe9820ca",
     "showTitle": true,
     "title": "supply-chains-latest is 3.11GB"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n*** Processing table *****.supply-chains-latest ***\nThe destination table '*****.supply-chains-latest' doesn't exist. Creating it and doing a full refresh\nCreating a hash column to uniquely identify records. Might take some time depending on the table size\nMade a full refresh of '*****.supply-chains-latest' in Databricks, and added a 'dbr_hash' column for uniquely identifying records\nRegistered the table property of 'supply-chains-latest': 'source_timestamp'= 2023-04-06 09:14:22.801232\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "\u001B[0;32m<command-338952632807930>\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[0mtable\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"supply-chains-latest\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m----> 5\u001B[0;31m \u001B[0mresults\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprocess_table\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtable\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcatalog\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrefresh_field\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"last_refresh_time\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\n",
       "\u001B[0;32m<command-338952632807917>\u001B[0m in \u001B[0;36mprocess_table\u001B[0;34m(table, schema, catalog, refresh_field, big_table)\u001B[0m\n",
       "\u001B[1;32m    138\u001B[0m         \u001B[0mregister_table_properties\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtable\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcatalog\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    139\u001B[0m         \u001B[0mregister_source_timestamp_property\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtable\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcatalog\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msource_time\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 140\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mdf_source\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhash_column\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m    141\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    142\u001B[0m     \u001B[0;31m# If not doing a full refresh, continue with comparisons\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'hash_column' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-338952632807930>\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mtable\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"supply-chains-latest\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mresults\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprocess_table\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtable\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcatalog\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrefresh_field\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"last_refresh_time\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m<command-338952632807917>\u001B[0m in \u001B[0;36mprocess_table\u001B[0;34m(table, schema, catalog, refresh_field, big_table)\u001B[0m\n\u001B[1;32m    138\u001B[0m         \u001B[0mregister_table_properties\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtable\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcatalog\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    139\u001B[0m         \u001B[0mregister_source_timestamp_property\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtable\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcatalog\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msource_time\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 140\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mdf_source\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhash_column\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    141\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    142\u001B[0m     \u001B[0;31m# If not doing a full refresh, continue with comparisons\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'hash_column' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'hash_column' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "catalog = \"****\"\n",
    "schema = \"*****\"\n",
    "table = \"supply-chains-latest\"\n",
    "\n",
    "results = process_table(table, schema, catalog, refresh_field=\"last_refresh_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73143325-70ac-4f7b-810e-9f851cab75b1",
     "showTitle": true,
     "title": "supply-chains-latest is 3.11GB"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n*** Processing table *****.supply-chains-latest ***\nThe destination table '*****.supply-chains-latest' doesn't exist. Creating it and doing a full refresh\nProcessing big table: supply-chains-latest\nCreating a hash column to uniquely identify records. Might take some time depending on the table size\nMade a full refresh of '*****.supply-chains-latest' in Databricks, and added a 'dbr_hash' column for uniquely identifying records\nRegistered the table property of 'supply-chains-latest': 'source_timestamp'= 2023-04-06 09:14:22.801232\n"
     ]
    }
   ],
   "source": [
    "catalog = \"****\"\n",
    "schema = \"*****\"\n",
    "table = \"supply-chains-latest\"\n",
    "\n",
    "supply-chains-latest_dict = {\n",
    "    \"num_partitions\": 18, \n",
    "    \"column_partition\": \"row_number\",\n",
    "    \"lower_bound\": 1,\n",
    "    \"upper_bound\": 5000000\n",
    "}\n",
    "\n",
    "results = process_table(table, schema, catalog, refresh_field=\"last_refresh_time\", supply-chains-latest_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f5b2ba8-b6a6-4626-a7de-8f32452001fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcce99b5-8df3-43a4-b70d-2d878e3bf1b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "links_reference_id, last_refresh_time, version, link_id, country_of_production, commodity, year, biome, trader_group, trader_group_id, commitment, tbl_bighash, dbr_hash\n"
     ]
    }
   ],
   "source": [
    "col_list = []\n",
    "col_result = spark.sql(f\"SHOW COLUMNS IN *****postgres.*****.commitments\").collect()\n",
    "for value in col_result:\n",
    "    col_list.append(value['col_name'])\n",
    "col_string = \", \".join(col_list)\n",
    "print(col_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db8ef4c9-1c6a-4aed-be5a-86b0f9d000bf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Exploration queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b269cdb5-8bb2-49e6-a04c-1aa388452eed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "INSERT INTO *****.commitments\n",
    "SELECT links_reference_id+2, last_refresh_time, version, link_id, country_of_production, commodity, year+2, biome, trader_group, trader_group_id, commitment\n",
    "FROM *****.commitments \n",
    "WHERE links_reference_id = 1744 AND year = 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e175629-50af-4c41-9933-6203e94bac6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>426</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         426
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"__autoGeneratedAlias\":\"true\"}",
         "name": "count(1)",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT COUNT(*)\n",
    "FROM *****postgres.*****.commitments \n",
    "WHERE links_reference_id = 1746 AND year = 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61e70874-fa3d-4dab-a182-4ed99eab0fdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM ****.*****.commitments \n",
    "WHERE links_reference_id = 1744 and year = 2014\n",
    "LIMIT 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcbfcc15-4ade-4ba3-90e0-a3475c456543",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: DataFrame[num_affected_rows: bigint]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DELETE FROM *****postgres.*****.commitments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a8a58f2-7eaa-4ec4-8a58-2e78074bb9a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>16848</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         16848
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"__autoGeneratedAlias\":\"true\"}",
         "name": "count(1)",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT COUNT(*)\n",
    "FROM *****postgres.*****.regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3307a16b-b21d-493e-9b44-1ae8ba08ce60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 338952632807939,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "postgres_to_databricks_replication",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
