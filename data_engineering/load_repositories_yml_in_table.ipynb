{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a98f0eac-b461-4322-b99f-3b03492f748e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Load the repositories.yml catalog into a table\n",
    "This notebooks reads the `****/products/****_internal/repositories.yml` from the **** repository and loads it into a couple of tables in `****.default`:\n",
    "- The table `bronze_repositories` saves the nested structure as it is (a deep nested structure).\n",
    "- The table `silver_repositories` transforms a couple of nested structures into columns and array elements into rows, so its easier to grasp and query\n",
    "\n",
    "It converts the yaml into a json and then loads the json into a table. Requires installing PyYAML, and for the moment, writing a temporary json file in a cloud location (so its easier to read from spark).\n",
    "\n",
    "Have in mind how to work with files in repositories [link here](https://docs.databricks.com/_extras/notebooks/source/files-in-repos.html) . This script currently needs to be executed from Databricks with access to read from the **** repo.\n",
    "\n",
    "## To-do\n",
    "* Only regenerate the tables if there are changes in `repositories.yml` (modification time and file size)\n",
    "* Keep track of which repositories have been loaded, and if there are some based on outdated csvs? (based on the csvs modification time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "deae0f3a-947d-4cf0-9115-d7215dce9d5f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Install PyYAML for transforming yaml into json\n",
    "This is needed to load the yaml into a json format that spark can easily load into a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1dc3490-0ba5-4c42-91ad-8faebd122dec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install PyYAML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12282c8d-364e-44a0-9aca-0be887f4d569",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Import required libraries and define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8d37953-862b-4c4a-8b79-ae1585a68d2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, StructField, StructType\n",
    "import yaml\n",
    "import json\n",
    "import time\n",
    "\n",
    "data_bucket = \"s3a://****\" # Will save temporary json here\n",
    "catalog = \"****\"\n",
    "db = \"default\"\n",
    "\n",
    "# github path, assuming we are standing in ****/products/delta_lake\n",
    "repositories_path = \"../../****_internal/repositories.yml\"\n",
    "\n",
    "nested_table = \"bronze_repositories\" # Name of the table for saving the yaml file structure as it is\n",
    "flat_table = \"silver_repositories\" # Name of the table flattening the structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3e7815f-2cf8-4d03-99ed-91e8f831775b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96a2ba3e-6473-4912-bc96-0b6b7f631e55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_temp_json(string_content, file_name, data_bucket):\n",
    "    \"\"\"Creates a temporary file based on the string provided and places it in s3:{data_bucket}/tmp/file_name_TSTIMESTAMP.json\"\"\"\n",
    "    current_time = int(time.time())\n",
    "    filepath = f\"{data_bucket}/tmp/{file_name}_TS{current_time}.json\"\n",
    "    print(f\"Writing temp file to {filepath}\")\n",
    "    dbutils.fs.put(filepath, string_content)\n",
    "    return(filepath)\n",
    "\n",
    "def flatten_df(nested_df):\n",
    "    \"\"\"Flatten a nested schema: transform nested structures into columns and array elements into rows\n",
    "    Taken from the example in\n",
    "    https://learn.microsoft.com/en-us/azure/synapse-analytics/how-to-analyze-complex-schema\n",
    "    \"\"\"\n",
    "    stack = [((), nested_df)]\n",
    "    columns = []\n",
    "\n",
    "    while len(stack) > 0:\n",
    "        parents, df = stack.pop()\n",
    "\n",
    "        flat_cols = [\n",
    "            col(\".\".join(parents + (c[0],))).alias(\"_\".join(parents + (c[0],)))\n",
    "            for c in df.dtypes\n",
    "            if c[1][:6] != \"struct\"\n",
    "        ]\n",
    "\n",
    "        nested_cols = [\n",
    "            c[0]\n",
    "            for c in df.dtypes\n",
    "            if c[1][:6] == \"struct\"\n",
    "        ]\n",
    "\n",
    "        columns.extend(flat_cols)\n",
    "\n",
    "        for nested_col in nested_cols:\n",
    "            projected_df = df.select(nested_col + \".*\")\n",
    "            stack.append((parents + (nested_col,), projected_df))\n",
    "\n",
    "    return nested_df.select(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe5f1176-d9c4-4970-a527-d73e0db36e8d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Create bronze (raw) table, with the structure as it is\n",
    "Note that as it has deep nested structures, they will not show in the Data Explorer UI. For exploring the data, the `silver_repositories` table will be better to look and query. In the future, a silver version with the data better cleaned would be nicer to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ea8858e-2197-4cd6-bf11-9c000ef094b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### PENDING: delete existing repositories temporary files (just to clean space)\n",
    "### (this is not done at the end of the script, as the dataframe will refer to the temp files while in use)\n",
    "\n",
    "# Open the YAML file\n",
    "with open(repositories_path, \"r\") as yaml_file:\n",
    "    yaml_string = yaml_file.read()\n",
    "\n",
    "# Removes the first 'repositories' line to prevent generating only one record\n",
    "lines = yaml_string.split(\"\\n\")\n",
    "lines = lines[1:]\n",
    "new_yaml_string = \"\\n\".join(lines)\n",
    "\n",
    "# Convert the YAML string to a Python object\n",
    "data = yaml.load(new_yaml_string, Loader=yaml.FullLoader)\n",
    "\n",
    "# Convert the Python object to a JSON string\n",
    "#json_string = json.dumps(data, separators=(',', ':')) # single line json version\n",
    "json_string = json.dumps(data, indent=4) # multi-line json version\n",
    "\n",
    "# Write the JSON string to a file\n",
    "temp_file = create_temp_json(json_string, \"repositories\", data_bucket)\n",
    "df = spark.read.option(\"mode\", \"PERMISSIVE\").option(\"multiline\", \"true\").json(temp_file, multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9947a00-969b-4e4b-ab30-5df3b4d49ce6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema() # Checking out the schema inferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e267f30-9c14-445d-a17e-e00bc8f317f9",
     "showTitle": true,
     "title": "Actual table creation"
    }
   },
   "outputs": [],
   "source": [
    "table_path = f\"{catalog}.{db}.{nested_table}\"\n",
    "df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"delta.columnMapping.mode\", 'name') \\\n",
    "    .option(\"delta.minReaderVersion\", 2) \\\n",
    "    .option(\"delta.minWriterVersion\", 5) \\\n",
    "    .saveAsTable(table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb5365ee-a91e-41e2-8c13-d7bccb91ce6f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Create flat version of the table\n",
    "* Flatten the structure into a new dataframe\n",
    "* Save the dataframe into a table\n",
    "* Add a new column that will include all the s3_objects of each record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc1398d6-33ca-420b-9c41-c9f860ec35d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_flat = flatten_df(df) # Flatten the dataframe\n",
    "\n",
    "# Remove dashes as they cause problem when used in column names\n",
    "df_flat = df_flat.select([col(c).alias(c.replace(\"-\", \"_\")) for c in df_flat.columns])\n",
    "\n",
    "# Save the table\n",
    "table_path = f\"{catalog}.{db}.{flat_table}\"\n",
    "df_flat.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"delta.columnMapping.mode\", 'name') \\\n",
    "    .option(\"delta.minReaderVersion\", 2) \\\n",
    "    .option(\"delta.minWriterVersion\", 5) \\\n",
    "    .saveAsTable(table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11c1d014-9288-413e-a86e-ec8a0422ca36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify all the columns that include s3_objects\n",
    "df_col_names = spark.sql(\"SELECT column_name FROM ****.information_schema.columns \" \\\n",
    "f\" WHERE table_name = '{flat_table}' AND column_name LIKE '%s3_object'\")\n",
    "col_names_res = df_col_names.collect()\n",
    "column_names = ', '.join([f\"{col_names_res[i][0]}\" for i in range(len(col_names_res))])\n",
    "column_names_q = ', '.join([f\"\\\"{col_names_res[i][0]}\\\"\" for i in range(len(col_names_res))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "852e8fca-147b-4c54-8ca2-39f14d521717",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Temporary solution for adding a column with all csvs of the repository\n",
    "To generate an 'all_s3_records' column containing a list of all related s3_objects from a record, here we're first adding a temp column including an array of the contents of all *s3_object columns, and then the final column that excludes all null values.\n",
    "\n",
    "For some reason, haven't been able to pass the array of column names as a string parameter, and instead have to add them manually (copy/pasting them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07adb6a6-a6cc-4471-a35d-72440c294401",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "column_names_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12e11d4c-cb77-4614-9dff-6a532f5327cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_extracol = (\n",
    "    df_flat\n",
    "    .withColumn(\"temp\", f.array(\"external_tables_1989_options_s3_object\", \"external_tables_1997_options_s3_object\", \"external_tables_2017_options_s3_object\", \"external_tables_2008_options_s3_object\", \"external_tables_1995_options_s3_object\", \"external_tables_2006_options_s3_object\", \"external_tables_2010_options_s3_object\", \"external_tables_2000_options_s3_object\", \"external_tables_2012_options_s3_object\", \"external_tables_2013_2019_options_s3_object\", \"external_tables_2014_options_s3_object\", \"external_tables_1992_options_s3_object\", \"external_tables_QA_options_s3_object\", \"external_tables_2005_options_s3_object\", \"external_tables_2007_options_s3_object\", \"external_tables_2009_options_s3_object\", \"external_tables_all_options_s3_object\", \"external_tables_2020_options_s3_object\", \"external_tables_2003_options_s3_object\", \"external_tables_data_options_s3_object\", \"external_tables_2002_options_s3_object\", \"external_tables_1996_options_s3_object\", \"external_tables_1991_options_s3_object\", \"external_tables_2019_options_s3_object\", \"external_tables_1994_options_s3_object\", \"external_tables_2015_options_s3_object\", \"external_tables_1998_options_s3_object\", \"external_tables_1990_options_s3_object\", \"external_tables_1999_options_s3_object\", \"external_tables_2013_options_s3_object\", \"external_tables_2011_options_s3_object\", \"external_tables_1988_options_s3_object\", \"external_tables_1993_options_s3_object\", \"external_tables_2018_options_s3_object\", \"external_tables_2004_options_s3_object\", \"external_tables_2016_options_s3_object\", \"external_tables_2017_ESTIMATES_options_s3_object\", \"external_tables_2001_options_s3_object\"))\n",
    "    .withColumn(\"all_s3_records\", f.expr(\"FILTER(temp, x -> x is not null)\"))\n",
    "    .drop(\"temp\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a628fb33-d3e3-499a-a4f6-92894afdc0f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Overwrite the flat table, now with the extra 'all_s3_records' column\n",
    "table_path = f\"{catalog}.{db}.{flat_table}\"\n",
    "df_with_extracol.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .option(\"delta.columnMapping.mode\", 'name') \\\n",
    "    .option(\"delta.minReaderVersion\", 2) \\\n",
    "    .option(\"delta.minWriterVersion\", 5) \\\n",
    "    .saveAsTable(table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9c3b144-0c69-44a9-a2e8-a4f534d69615",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# From here on, debugging cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9247135d-b61f-4faf-a314-c7fc4971235b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "LIST 's3a://****/badRecords/brazil/auxiliary/20230126T171755/bad_records/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9553403-5df5-4b15-8783-fbc7ea5b661e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.head(\"s3://****/badRecords/brazil/auxiliary/20230126T171755/bad_records/part-00000-b5d07ed1-faf4-4e9b-b2fe-91512f022bec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca2e3ad2-c583-4de1-aa64-4b56ba1e5993",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) FROM brazil.auxiliary.bronze_cnpj_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca16fbfc-0570-4bd8-89cc-176503efa0fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "load_repositories_yml_in_table",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
